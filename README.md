# Generador de CÃ³digo Terraform con LangChain y Hugging Face

Proyecto en Python que utiliza LangChain y modelos de Hugging Face para generar cÃ³digo Terraform a partir de descripciones en lenguaje natural. Incluye capacidad de entrenar modelos personalizados con tus propios datos.

## ğŸ“‹ Tabla de Contenidos

1. [InstalaciÃ³n](#instalaciÃ³n)
2. [Uso RÃ¡pido](#uso-rÃ¡pido)
3. [Entrenamiento del Modelo](#entrenamiento-del-modelo)
4. [Estructura del Proyecto](#estructura-del-proyecto)
5. [SoluciÃ³n de Problemas](#soluciÃ³n-de-problemas)

---

## ğŸš€ InstalaciÃ³n

### OpciÃ³n 1: InstalaciÃ³n AutomÃ¡tica (Recomendada)

```bash
python install_dependencies.py
```

Este script instala las dependencias en el orden correcto y maneja conflictos automÃ¡ticamente.

### OpciÃ³n 2: InstalaciÃ³n Manual

1. **Dependencias bÃ¡sicas:**

```bash
pip install python-dotenv sentencepiece "protobuf>=3.20.0,<5.0.0"
```

2. **Ecosistema Hugging Face:**

```bash
pip install "huggingface-hub>=0.16.0" "transformers>=4.30.0" "datasets>=2.12.0" "accelerate>=0.20.0"
```

3. **LangChain:**

```bash
pip install "langchain>=0.1.0" "langchain-community>=0.0.10" "langchain-core>=0.1.0"
```

4. **PyTorch (segÃºn tu sistema):**

```bash
# Para CPU
pip install torch

# Para GPU NVIDIA (CUDA 11.8)
pip install torch --index-url https://download.pytorch.org/whl/cu118

# Para GPU NVIDIA (CUDA 12.1)
pip install torch --index-url https://download.pytorch.org/whl/cu121
```

### Verificar InstalaciÃ³n

```bash
python -c "from terraform_generator import TerraformGenerator; print('âœ“ InstalaciÃ³n correcta')"
```

---

## âš¡ Uso RÃ¡pido

### 1. Probar con Modelo Pre-entrenado

**âš ï¸ Importante:** El modelo pre-entrenado NO estÃ¡ especializado en Terraform y puede generar cÃ³digo de baja calidad o basura. Para mejores resultados, [entrena el modelo](#entrenamiento-del-modelo).

#### Modo Interactivo:

```bash
python main.py
```

#### Desde lÃ­nea de comandos:

```bash
python generate_terraform.py --prompt "Crear un bucket S3 con versionado habilitado"
```

#### Desde Python:

```python
from terraform_generator import TerraformGenerator

generator = TerraformGenerator()
codigo = generator.generate("Crear un bucket S3")
print(codigo)
```

### 2. Usar Modelo Entrenado (Recomendado)

```bash
python main.py --model_path models/terraform_generator
```

---

## ğŸ“ Entrenamiento del Modelo

### Â¿Por quÃ© entrenar?

| Aspecto         | Modelo Pre-entrenado | Modelo Entrenado          |
| --------------- | -------------------- | ------------------------- |
| Calidad         | âš ï¸ Variable/Basura   | âœ… CÃ³digo vÃ¡lido          |
| EspecializaciÃ³n | âŒ No                | âœ… SÃ­, en Terraform       |
| Listo para usar | âœ… Inmediato         | â±ï¸ Requiere entrenamiento |

**El modelo pre-entrenado puede generar cÃ³digo basura.** Para obtener cÃ³digo Terraform de calidad, **debes entrenar el modelo** con tus datos.

### Paso 1: Preparar Datos de Entrenamiento

Los datos deben estar en `training_data/` con formato JSON:

```json
[
  {
    "description": "Crear un bucket S3 con versionado habilitado",
    "terraform_code": "resource \"aws_s3_bucket\" \"example\" {\n  bucket = \"my-bucket\"\n  versioning {\n    enabled = true\n  }\n}"
  }
]
```

**Ya tienes ejemplos en:** `training_data/example_training_data.json`

### Paso 2: Entrenar el Modelo

```bash
python train_model.py --data_dir training_data/ --output_dir models/terraform_generator
```

**Opciones avanzadas:**

```bash
python train_model.py \
  --data_dir training_data/ \
  --output_dir models/terraform_generator \
  --epochs 5 \
  --batch_size 4 \
  --learning_rate 5e-5
```

**Tiempo estimado:**

- 5-10 ejemplos: 15-30 minutos
- 50+ ejemplos: 1-3 horas o mÃ¡s
- Requiere CPU o GPU

### Paso 3: Usar el Modelo Entrenado

```bash
# Modo interactivo
python main.py --model_path models/terraform_generator

# Desde lÃ­nea de comandos
python generate_terraform.py \
  --prompt "Crear una instancia EC2" \
  --model_path models/terraform_generator \
  --output mi_codigo.tf
```

### Verificar si Tienes Modelo Entrenado

```bash
dir models\terraform_generator
```

Si la carpeta estÃ¡ vacÃ­a o no existe, no tienes un modelo entrenado aÃºn.

---

## ğŸ“ Estructura del Proyecto

```
.
â”œâ”€â”€ main.py                      # Script interactivo principal
â”œâ”€â”€ generate_terraform.py        # GeneraciÃ³n desde lÃ­nea de comandos
â”œâ”€â”€ train_model.py              # Script de entrenamiento
â”œâ”€â”€ terraform_generator.py      # MÃ³dulo principal con la lÃ³gica
â”œâ”€â”€ install_dependencies.py     # InstalaciÃ³n automÃ¡tica
â”œâ”€â”€ requirements.txt            # Dependencias
â”œâ”€â”€ training_data/             # Datos de entrenamiento
â”‚   â””â”€â”€ example_training_data.json
â””â”€â”€ models/                     # Modelos entrenados (se crea al entrenar)
```

---

## ğŸ”§ SoluciÃ³n de Problemas

### Error: "CÃ³digo basura generado"

**Problema:** El modelo pre-entrenado genera cÃ³digo invÃ¡lido como:

```
HAIL: dongeran la colonlas s3 en terraform """ # TODO: Fix this...
```

**SoluciÃ³n:** Esto es normal con modelos pre-entrenados. **Entrena el modelo** con tus datos:

```bash
python train_model.py --data_dir training_data/ --output_dir models/terraform_generator
```

### Error: "Conflictos de dependencias"

**SoluciÃ³n:** Usa el script de instalaciÃ³n automÃ¡tica:

```bash
python install_dependencies.py
```

O instala manualmente en el orden correcto (ver [InstalaciÃ³n](#instalaciÃ³n)).

### Error: "Modelo no encontrado"

- Los modelos se descargan automÃ¡ticamente la primera vez
- Verifica tu conexiÃ³n a internet
- Algunos modelos pueden requerir autenticaciÃ³n en Hugging Face

### Error: "Out of memory"

- Usa un modelo mÃ¡s pequeÃ±o: `--model_name gpt2`
- Reduce batch_size: `--batch_size 2`
- Cierra otras aplicaciones

### GeneraciÃ³n lenta

- El primer uso es mÃ¡s lento (descarga del modelo)
- Considera usar GPU si estÃ¡ disponible
- Los modelos entrenados son mÃ¡s rÃ¡pidos

### Warnings sobre bitsandbytes/torch

Son normales si no tienes GPU. No afectan la funcionalidad, el proyecto funciona en CPU.

---

## ğŸ“ Ejemplos PrÃ¡cticos

### Generar y Guardar CÃ³digo

```bash
python generate_terraform.py \
  --prompt "Crear un bucket S3 con encriptaciÃ³n" \
  --output s3_bucket.tf
```

### Generar con Contexto

```bash
python generate_terraform.py \
  --prompt "Crear un grupo de seguridad" \
  --context "Para una instancia EC2 en la VPC vpc-12345"
```

---

## ğŸ¯ Modelos Soportados

Por defecto se usa:

- `microsoft/CodeGPT-small-py` (pre-entrenado)
- `gpt2` (para pruebas rÃ¡pidas)

Puedes especificar otros modelos:

```bash
python main.py --model_name "Salesforce/codegen-350M-mono"
```

---

## âš™ï¸ ConfiguraciÃ³n Avanzada

### Variables de Entorno

Crear archivo `.env`:

```
HUGGINGFACE_API_TOKEN=tu_token_aqui
```

### Personalizar ParÃ¡metros

En `terraform_generator.py` puedes ajustar:

- `temperature`: Controla la creatividad (0.1-1.0)
- `max_length`: Longitud mÃ¡xima de generaciÃ³n
- `device`: CPU o CUDA

---

## ğŸ“š PrÃ³ximos Pasos

1. âœ… **Probar** con modelo pre-entrenado: `python main.py`
2. ğŸ“Š **Agregar mÃ¡s datos** de entrenamiento en `training_data/`
3. ğŸ“ **Entrenar el modelo**: `python train_model.py --data_dir training_data/ --output_dir models/terraform_generator`
4. ğŸš€ **Usar modelo entrenado**: `python main.py --model_path models/terraform_generator`

---

## âš ï¸ Notas Importantes

- **Primera ejecuciÃ³n:** Los modelos se descargan automÃ¡ticamente (puede tomar varios minutos)
- **Recursos:** El entrenamiento requiere recursos computacionales significativos
- **GPU:** Opcional pero recomendado para entrenamiento y modelos grandes
- **Datos:** Mientras mÃ¡s ejemplos de entrenamiento tengas, mejor serÃ¡ el modelo
- **Calidad:** El modelo pre-entrenado nunca serÃ¡ tan bueno como uno entrenado especÃ­ficamente para Terraform

---

## ğŸ“„ Licencia

Este proyecto es de cÃ³digo abierto. SiÃ©ntete libre de usarlo y modificarlo segÃºn tus necesidades.

---

**Â¿Necesitas ayuda?** Revisa la secciÃ³n [SoluciÃ³n de Problemas](#soluciÃ³n-de-problemas) o consulta los comentarios en el cÃ³digo.
